model,metric,mean,ci_lower,ci_upper,std,n,median,min,max
ChatGPT_4o,semantic_accuracy,0.5438999999999999,0.501250271963006,0.5865497280369939,0.026803109272371152,4,0.53755,0.5193,0.5812
ChatGPT_4o,reasoning_quality,0.2975,0.04881569582552239,0.5461843041744776,0.15628499608087784,4,0.36,0.07,0.4
ChatGPT_4o,domain_knowledge_score,0.53,0.43452661084147215,0.6254733891585279,0.06000000000000001,4,0.52,0.48,0.6
ChatGPT_4o,factual_correctness,0.46025,-0.006793132958538617,0.9272931329585385,0.29351202701081947,4,0.3205,0.3,0.9
ChatGPT_4o,depth_score,0.375,0.13851286687562364,0.6114871331243763,0.14861971605409557,4,0.37224999999999997,0.2222,0.5333
ChatGPT_4o,biobank_specificity,0.458325,0.20436085385579272,0.7122891461442072,0.15960309886298155,4,0.41664999999999996,0.3333,0.6667
ChatGPT_4o,coverage_score,0.46025,-0.006793132958538617,0.9272931329585385,0.29351202701081947,4,0.3205,0.3,0.9
ChatGPT_o1_pro,semantic_accuracy,0.5919000000000001,0.4732825940982909,0.7105174059017093,0.0745447963755128,4,0.58125,0.5222,0.6829
ChatGPT_o1_pro,reasoning_quality,0.585,0.22553380283985985,0.94446619716014,0.225905584998099,4,0.63,0.28,0.8
ChatGPT_o1_pro,domain_knowledge_score,0.8400000000000001,0.7360617389099731,0.9439382610900271,0.0653197264742181,4,0.84,0.76,0.92
ChatGPT_o1_pro,factual_correctness,0.7760750000000001,0.551622746201292,1.000527253798708,0.14105642783415914,4,0.79445,0.6154,0.9
ChatGPT_o1_pro,depth_score,0.76945,0.4213735209341718,1.1175264790658281,0.21874774665506083,4,0.79445,0.4889,1.0
ChatGPT_o1_pro,biobank_specificity,0.833325,0.4582701051909335,1.2083798948090665,0.23570226098477148,4,0.91665,0.5,1.0
ChatGPT_o1_pro,coverage_score,0.7760750000000001,0.551622746201292,1.000527253798708,0.14105642783415914,4,0.79445,0.6154,0.9
Claude_Sonnet,semantic_accuracy,0.538525,0.4105614778860809,0.6664885221139192,0.08041833849730212,4,0.544,0.436,0.6301
Claude_Sonnet,reasoning_quality,0.3825,0.1550886575899816,0.6099113424100184,0.14291605927956452,4,0.41000000000000003,0.21,0.5
Claude_Sonnet,domain_knowledge_score,0.64,0.4134718117706133,0.8665281882293867,0.1423610433604175,4,0.66,0.48,0.76
Claude_Sonnet,factual_correctness,0.37370000000000003,0.25945512723456743,0.48794487276543264,0.07179688944051361,4,0.36665000000000003,0.3,0.4615
Claude_Sonnet,depth_score,0.33055,0.24842207254236418,0.4126779274576359,0.05161307973760141,4,0.3222,0.2778,0.4
Claude_Sonnet,biobank_specificity,0.54165,0.03960384740181566,1.0436961525981843,0.31550958252748307,4,0.41664999999999996,0.3333,1.0
Claude_Sonnet,coverage_score,0.37370000000000003,0.25945512723456743,0.48794487276543264,0.07179688944051361,4,0.36665000000000003,0.3,0.4615
Gemini,semantic_accuracy,0.5944499999999999,0.4115902566176831,0.7773097433823167,0.11491772419141154,4,0.546,0.5215,0.7643
Gemini,reasoning_quality,0.455,0.18200743842317163,0.7279925615768283,0.17156145643277027,4,0.41000000000000003,0.3,0.7
Gemini,domain_knowledge_score,0.76,0.6560617389099731,0.8639382610900269,0.06531972647421805,4,0.76,0.68,0.84
Gemini,factual_correctness,0.790375,0.4180926095142967,1.1626573904857034,0.2339598879437812,4,0.8500000000000001,0.4615,1.0
Gemini,depth_score,0.37502500000000005,0.1282090924108517,0.6218409075891485,0.1551108071670056,4,0.3167,0.2667,0.6
Gemini,biobank_specificity,0.60415,0.2389895481543628,0.9693104518456371,0.22948412436012508,4,0.625,0.3333,0.8333
Gemini,coverage_score,0.790375,0.4180926095142967,1.1626573904857034,0.2339598879437812,4,0.8500000000000001,0.4615,1.0
Mistral_Large,semantic_accuracy,0.5177499999999999,0.3999242143706275,0.6355757856293723,0.07404730469999116,4,0.5153,0.4305,0.6099
Mistral_Large,reasoning_quality,0.2375,0.1658949581311041,0.30910504186889587,0.04499999999999999,4,0.22499999999999998,0.2,0.3
Mistral_Large,domain_knowledge_score,0.53,0.27874121183626477,0.7812587881637353,0.15790292376436013,4,0.48,0.4,0.76
Mistral_Large,factual_correctness,0.46795,0.27271457067901317,0.6631854293209868,0.12269519142981927,4,0.46925,0.3333,0.6
Mistral_Large,depth_score,0.37775,0.2119020236148857,0.5435979763851142,0.10422672402028187,4,0.3333,0.3111,0.5333
Mistral_Large,biobank_specificity,0.37497499999999995,0.24234655022727827,0.5076034497727216,0.08335000000000001,4,0.3333,0.3333,0.5
Mistral_Large,coverage_score,0.46795,0.27271457067901317,0.6631854293209868,0.12269519142981927,4,0.46925,0.3333,0.6
Meta_Llama,semantic_accuracy,0.576225,0.45126676995041953,0.7011832300495804,0.07852967061351183,4,0.5612,0.4979,0.6846
Meta_Llama,reasoning_quality,0.29500000000000004,0.0914312040399575,0.49856879596004255,0.12793227374930327,4,0.32,0.14,0.4
Meta_Llama,domain_knowledge_score,0.6100000000000001,0.45960345520777757,0.7603965447922226,0.0945163125250522,4,0.64,0.48,0.68
Meta_Llama,factual_correctness,0.58185,0.24314269921957332,0.9205573007804266,0.21285971123410524,4,0.51925,0.4,0.8889
Meta_Llama,depth_score,0.394425,0.07502848910065046,0.7138215108993495,0.20072389618578051,4,0.37775000000000003,0.2222,0.6
Meta_Llama,biobank_specificity,0.541675,0.4090465502272784,0.6743034497727216,0.08334999999999998,4,0.5,0.5,0.6667
Meta_Llama,coverage_score,0.58185,0.24314269921957332,0.9205573007804266,0.21285971123410524,4,0.51925,0.4,0.8889
DeepThink_R1,semantic_accuracy,0.5591250000000001,0.3933362762883376,0.7249137237116625,0.10418948683368524,4,0.5765,0.4368,0.6467
DeepThink_R1,reasoning_quality,0.3225,0.2135079895356638,0.4314920104643362,0.06849574196011507,4,0.32499999999999996,0.24,0.4
DeepThink_R1,domain_knowledge_score,0.62,0.4221074466300778,0.8178925533699222,0.12436505404118421,4,0.6200000000000001,0.48,0.76
DeepThink_R1,factual_correctness,0.5952999999999999,0.17019801166130244,1.0204019883386974,0.26715422512099635,4,0.69615,0.2,0.7889
DeepThink_R1,depth_score,0.37777499999999997,0.04351867919928554,0.7120313208007144,0.2100625045869284,4,0.31110000000000004,0.2222,0.6667
DeepThink_R1,biobank_specificity,0.458325,0.3256965502272783,0.5909534497727217,0.08335000000000001,4,0.5,0.3333,0.5
DeepThink_R1,coverage_score,0.6703,0.2571799984174733,1.0834200015825268,0.25962417709707497,4,0.7461500000000001,0.3,0.8889
